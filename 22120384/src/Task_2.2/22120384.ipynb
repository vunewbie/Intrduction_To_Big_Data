{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weekly Sales Data Processing With RDDs in PySpark\n",
    "\n",
    "This notebook presents a solution to process product sales data on a weekly basis using PySpark's RDD (Resilient Distributed Dataset) abstraction. The dataset consists of shipping records, and the goal is to calculate the total quantity of products shipped in each week.\n",
    "\n",
    "The workflow includes:\n",
    "- Initializing a Spark session and loading the dataset.\n",
    "- Removing the header and extracting relevant fields.\n",
    "- Filtering orders with a shipping status that starts with \"Shipped\".\n",
    "- Identifying Mondays as weekly boundaries.\n",
    "- Aggregating quantities shipped per SKU for each week.\n",
    "- Exporting the final result to a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Initialize SparkSession and SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Weekly-Sales\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines before removing header: 128976\n",
      "Number of lines after removing header: 128975\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(\"file:///D:/Intrduction_To_Big_Data/22120384/src/Task_2.2/asr.csv\")\n",
    "\n",
    "print(f\"Number of lines before removing header: {lines.count()}\")\n",
    "\n",
    "# remove header\n",
    "header = lines.first()\n",
    "lines = lines.filter(lambda x: x != header)\n",
    "\n",
    "print(f\"Number of lines after removing header: {lines.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Extract Relevant Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Set', '04-30-22', 'Cancelled', '0'),\n",
       " ('kurta', '04-30-22', 'Shipped - Delivered to Buyer', '1'),\n",
       " ('kurta', '04-30-22', 'Shipped', '1'),\n",
       " ('Western Dress', '04-30-22', 'Cancelled', '0'),\n",
       " ('Top', '04-30-22', 'Shipped', '1')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the relevant columns\n",
    "extracted_data = lines.map(lambda line:(\n",
    "    line.split(\",\")[9],  # Category -> SKU\n",
    "    line.split(\",\")[2],  # Date\n",
    "    line.split(\",\")[3],  # Status\n",
    "    line.split(\",\")[13],  # Quantity\n",
    "))\n",
    "extracted_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Define Date Conversion Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Convert date string to datetime object\n",
    "def safe_parse_date(date_str):\n",
    "    try:\n",
    "        return datetime.strptime(date_str.strip(), \"%m-%d-%y\").date()\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "# Convert quantity string to integer\n",
    "def safe_int(value):\n",
    "    try:\n",
    "        return int(value)\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Filter Shipped Orders and Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kurta', datetime.date(2022, 4, 30), 1),\n",
       " ('kurta', datetime.date(2022, 4, 30), 1),\n",
       " ('Top', datetime.date(2022, 4, 30), 1),\n",
       " ('Set', datetime.date(2022, 4, 30), 1),\n",
       " ('Set', datetime.date(2022, 4, 30), 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the data for shipped items have status start with \"Shipped\"\n",
    "shipped_data = extracted_data.filter(lambda x: x[2].startswith(\"Shipped\"))\n",
    "\n",
    "# Convert data types\n",
    "parsed_data =  shipped_data.map(lambda x: (x[0], safe_parse_date(x[1]), safe_int(x[3])))\n",
    "\n",
    "# Find the average quantity of shipped items\n",
    "valid_quantities = parsed_data.filter(lambda x: x[2] is not None)\\\n",
    "                              .map(lambda x: x[2])\n",
    "avg_quantity = int(round(valid_quantities.mean()))\n",
    "\n",
    "# Remove None values in date and replace none values in quantity with average quantity\n",
    "transformed_data = parsed_data.filter(lambda x: x[1] is not None)\\\n",
    "                              .map(lambda x: (x[0], x[1], x[2] if x[2] is not None else avg_quantity))\n",
    "\n",
    "transformed_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Identify Mondays and Aggregate Weekly Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all mondays in the dataset\n",
    "dates = transformed_data.map(lambda x: x[1])\\\n",
    "                        .distinct()\\\n",
    "                        .sortBy(lambda x: x)\\\n",
    "                        .collect()\n",
    "mondays = [date for date in dates if date.weekday() == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Calculate the total quantity of products shipped in each week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of tuples (SKU, week_start_date, quantity)\n",
    "loaded_data = []\n",
    "\n",
    "for monday in mondays:\n",
    "    # Filter the shipped data for the current week\n",
    "    start_date = monday - timedelta(days=7)\n",
    "    end_date = monday\n",
    "    weekly_data = transformed_data.filter(lambda x: start_date <= x[1] < end_date)\n",
    "    \n",
    "    # Sum the quantities for each SKU in the current week\n",
    "    weekly_sales = weekly_data.map(lambda x: (x[0], x[2]))\\\n",
    "                              .reduceByKey(lambda x, y: x + y)\\\n",
    "                              .map(lambda x: (monday.strftime(\"%d-%m-%Y\"), x[0], x[1]))        \n",
    "\n",
    "    loaded_data.extend(weekly_sales.collect())\n",
    "\n",
    "# Sort by date\n",
    "loaded_data.sort(key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to CSV file\n",
    "header = [\"report_date\", \"sku\", \"total_quantity\"]\n",
    "output_file = \"./output.csv\"\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(\",\".join(header) + \"\\n\")\n",
    "    for row in loaded_data:\n",
    "        f.write(\",\".join(map(str, row)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Stop SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
